{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_io as tfio\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout, Bidirectional, Add, Flatten, Normalization, Input, Resizing, Conv2D, MaxPooling2D\n",
    "\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras import Sequential\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from src.const import AUDIO_PATH, MAIN_LABELS, BATCH_SIZE, VALIDATION_SPLIT, SEED\n",
    "\n",
    "from src.preprocess import load_and_preprocess, load_augmented_data\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unknown vs known task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load augmented data\n",
      "Found 64721 files belonging to 30 classes.\n",
      "Using 51777 files for training.\n",
      "Using 12944 files for validation.\n",
      "Create data with only main classes\n",
      "Create binary dataset\n",
      "Create main dataset\n",
      "Found 64721 files belonging to 30 classes.\n",
      "Using 51777 files for training.\n",
      "Using 12944 files for validation.\n",
      "Transform to spectograms\n",
      "Transform to numpy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing dataset: 100%|██████████| 70773/70773 [02:35<00:00, 456.06it/s] \n",
      "Processing dataset: 100%|██████████| 12944/12944 [00:15<00:00, 818.20it/s]\n",
      "Processing dataset: 100%|██████████| 37992/37992 [00:44<00:00, 844.44it/s] \n",
      "Processing dataset: 100%|██████████| 12944/12944 [00:15<00:00, 834.84it/s]\n"
     ]
    }
   ],
   "source": [
    "train_ds_bin_X, train_ds_bin_y, val_ds_bin_X, val_ds_bin_y, train_ds_main_X, train_ds_main_y, val_ds_main_X, val_ds_main_y = load_and_preprocess(plot_samples = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_ds_bin_X[val_ds_bin_X == -np.inf] = np.min(train_ds_bin_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds_bin_X = torch.tensor(train_ds_bin_X, dtype=torch.float32).to(device)\n",
    "train_ds_bin_y = torch.tensor(train_ds_bin_y, dtype=torch.float32).to(device)\n",
    "val_ds_bin_X = torch.tensor(val_ds_bin_X, dtype=torch.float32).to(device)\n",
    "val_ds_bin_y = torch.tensor(val_ds_bin_y, dtype=torch.float32).to(device)\n",
    "\n",
    "train_dataset = TensorDataset(train_ds_bin_X, train_ds_bin_y)\n",
    "train_loader = DataLoader(train_dataset, batch_size=256, shuffle=True)\n",
    "val_dataset = TensorDataset(val_ds_bin_X, val_ds_bin_y)\n",
    "val_loader = DataLoader(val_dataset, batch_size=256, shuffle=False)\n",
    "\n",
    "del train_ds_bin_X, train_ds_bin_y, val_ds_bin_X, val_ds_bin_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiLSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=input_size, \n",
    "            hidden_size=hidden_size, \n",
    "            batch_first=True, \n",
    "            bidirectional=True, \n",
    "            num_layers=2\n",
    "        )\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        self.fc1 = nn.Linear(2*hidden_size, 64)  # *2 because of bidirectional\n",
    "        self.fc2 = nn.Linear(64, output_size)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        out, _ = self.lstm(x)\n",
    "        out = torch.sum(out, dim=1)\n",
    "        out = self.dropout(out)\n",
    "        out = self.fc1(out)\n",
    "        out = self.dropout(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc2(out)\n",
    "        out = self.sigmoid(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 128\n",
    "hidden_size = 128\n",
    "output_size = 1\n",
    "\n",
    "model = BiLSTM(input_size, hidden_size, output_size).to(device)\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_val_loss = float('inf')\n",
    "patience = 5\n",
    "no_improve_count = 0\n",
    "\n",
    "# Training\n",
    "for epoch in range(50):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    correct_train = 0\n",
    "    total_train = 0\n",
    "\n",
    "    for batch_X, batch_y in tqdm(train_loader, f\"Epoch {epoch}\"):\n",
    "        model.zero_grad()\n",
    "        outputs = model(batch_X)\n",
    "        loss = criterion(outputs, batch_y.unsqueeze(1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Loss\n",
    "        train_loss += loss.item() * batch_X.size(0)\n",
    "\n",
    "        # Accuracy\n",
    "        predicted = (outputs > 0.5).float()\n",
    "        correct_train += (predicted == batch_y.unsqueeze(1)).sum().item()\n",
    "        total_train += batch_y.size(0)\n",
    "\n",
    "    train_loss /= len(train_loader.dataset)\n",
    "    train_acc = correct_train / total_train\n",
    "    \n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    correct_val = 0\n",
    "    total_val = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_X, batch_y in val_loader:\n",
    "            outputs = model(batch_X)\n",
    "            loss = criterion(outputs, batch_y.unsqueeze(1))\n",
    "            \n",
    "            # Loss\n",
    "            val_loss += loss.item() * batch_X.size(0)\n",
    "\n",
    "            # Accuracy\n",
    "            predicted = (outputs > 0.5).float()\n",
    "            correct_val += (predicted == batch_y.unsqueeze(1)).sum().item()\n",
    "            total_val += batch_y.size(0)\n",
    "\n",
    "        val_loss /= len(val_loader.dataset)\n",
    "        val_acc = correct_val / total_val\n",
    "        \n",
    "    print(f'Epoch {epoch+1}/{50}, Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}, Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}')\n",
    "    \n",
    "    # Early stopping check\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        no_improve_count = 0\n",
    "    else:\n",
    "        no_improve_count += 1\n",
    "    \n",
    "    if no_improve_count >= patience:\n",
    "        print('Early stopping')\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
